\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Schapire90thestrength,Freund:1995:BWL:220262.220446,Freund:1996:ENB:3091696.3091715}
\citation{Quinlan:1993:CPM:152181}
\citation{Freund:1996:ENB:3091696.3091715,Quinlan:1996:BBC:1892875.1892983}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{introduction}{{1}{1}{Introduction}{section.1}{}}
\citation{DBLP:conf/icml/KalyanakrishnanTAS12,NIPS2012_4640,COLT13}
\citation{icml2013_appel13}
\citation{Chen:2016:XST:2939672.2939785}
\@writefile{toc}{\contentsline {paragraph}{The Multi-Armed Bandit (MAB) Inspiration.}{2}{section*.2}}
\@writefile{toc}{\contentsline {paragraph}{Our Lower Bounds.}{2}{section*.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Related Work}{2}{subsection.1.1}}
\newlabel{related-work}{{1.1}{2}{Related Work}{subsection.1.1}{}}
\citation{4270071,PaulBiswajitAthithanEtAl}
\citation{busafekete:in2p3-00614564}
\citation{Escudero:2001:ULW:2387364.2387381}
\citation{Dubout:2014:ASL:2627435.2638580}
\citation{icml2013_karnin13}
\citation{FriedmanStochasticGB}
\citation{Friedman98additivelogistic}
\citation{NIPS2007_3321}
\citation{implementing-decision-trees-and-forests-on-a-gpu,articleWuEtAl}
\citation{icml2013_appel13}
\citation{icml2013_appel13}
\citation{icml2013_appel13}
\@writefile{toc}{\contentsline {section}{\numberline {2}Setup and Notation}{3}{section.2}}
\@writefile{toc}{\contentsline {paragraph}{A Generic Boosting Algorithm.}{3}{section*.4}}
\@writefile{toc}{\contentsline {paragraph}{Decision Tree.}{3}{section*.5}}
\@writefile{tdo}{\contentsline {todo}{hopefully we can show how to do this for info gain in a remark here}{3}{section*.6}}
\pgfsyspdfmark {pgfid1}{12972140}{6392708}
\pgfsyspdfmark {pgfid4}{35988437}{6384840}
\pgfsyspdfmark {pgfid5}{38101972}{6171525}
\citation{icml2013_appel13}
\@writefile{toc}{\contentsline {section}{\numberline {3}Algorithm}{4}{section.3}}
\newlabel{alg}{{3}{4}{Algorithm}{section.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces \texttt  {Adaptive-Pruning Stump}\relax }}{5}{figure.caption.7}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{adaptive-pruning-stump}{{1}{5}{\texttt {Adaptive-Pruning Stump}\relax }{figure.caption.7}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces \texttt  {Adaptive-Pruning Boost}\relax }}{5}{figure.caption.7}}
\newlabel{boosting}{{2}{5}{\texttt {Adaptive-Pruning Boost}\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Implementation Details.}{5}{section*.8}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Lower Bounds}{5}{section.4}}
\newlabel{lb}{{4}{5}{Lower Bounds}{section.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Weight Order Lower Bound.}{5}{section*.10}}
\citation{icml2013_appel13}
\citation{Platt:1999:FTS:299094.299105}
\citation{726791}
\citation{Lewis:2004:RNB:1005332.1005345}
\citation{991427}
\citation{Platt:1999:FTS:299094.299105}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Lower Bounds versus Upper Bounds. Datasets W4A (top) and A6A (bottom) were used with trees of depth 1. The y-axis is the \emph  {fraction of the gap} between the exact lower bound (at zero) and the full corpus size (at one) which an algorithm used in a given round. Non-cumulative example assessments are plotted for every 10 rounds. \relax }}{6}{figure.caption.9}}
\newlabel{fig:elb}{{1}{6}{Lower Bounds versus Upper Bounds. Datasets W4A (top) and A6A (bottom) were used with trees of depth 1. The y-axis is the \emph {fraction of the gap} between the exact lower bound (at zero) and the full corpus size (at one) which an algorithm used in a given round. Non-cumulative example assessments are plotted for every 10 rounds. \relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {paragraph}{Exact Lower Bound.}{6}{section*.11}}
\@writefile{toc}{\contentsline {paragraph}{Discussion.}{6}{section*.12}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{6}{section.5}}
\citation{Friedman98additivelogistic}
\citation{Escudero:2001:ULW:2387364.2387381}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The datasets used in our experiments.\relax }}{7}{table.caption.13}}
\newlabel{datasets}{{1}{7}{The datasets used in our experiments.\relax }{table.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Boosting Exact Greedy-Optimal Decision Trees}{7}{subsection.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces We report the total number of assessments at various boosting rounds used by the algorithms, as well as the weight order lower bound. In all of these experiments, our algorithm, \texttt  {AP Boost}, not only consistently beats \texttt  {Quick Boost} but it also almost matches the lower bound.\relax }}{7}{figure.caption.14}}
\newlabel{fig:wolb}{{2}{7}{We report the total number of assessments at various boosting rounds used by the algorithms, as well as the weight order lower bound. In all of these experiments, our algorithm, \texttt {AP Boost}, not only consistently beats \texttt {Quick Boost} but it also almost matches the lower bound.\relax }{figure.caption.14}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Computational Complexity for AdaBoost. All results are for 500 rounds of boosting except MNIST (300 rounds) and RCV1 (400 rounds).\relax }}{7}{table.caption.15}}
\newlabel{complexity-exact-results}{{2}{7}{Computational Complexity for AdaBoost. All results are for 500 rounds of boosting except MNIST (300 rounds) and RCV1 (400 rounds).\relax }{table.caption.15}{}}
\citation{Chen:2016:XST:2939672.2939785}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Boosting Approximate Decision Trees}{8}{subsection.5.2}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Computational Complexity for LazyBoost and Boosting with Weight Trimming. All results are for 500 rounds of boosting except MNIST (300 rounds) and RCV1 (400 rounds).\relax }}{8}{table.caption.16}}
\newlabel{complexity-approx-results}{{3}{8}{Computational Complexity for LazyBoost and Boosting with Weight Trimming. All results are for 500 rounds of boosting except MNIST (300 rounds) and RCV1 (400 rounds).\relax }{table.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{8}{section.6}}
\bibdata{paper, biblioEmilie}
\bibstyle{plainnat}
\@writefile{toc}{\contentsline {section}{\numberline {A}Additional Results}{9}{Appendix.1.A}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Train and Test Error for AdaBoost}{9}{subsection.1.A.1}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces AdaBoost results, reported at rounds 100, 300 and 500 (400 for RCV1).\relax }}{9}{table.caption.17}}
\newlabel{tbl:AdaBoostTestTrainErrors}{{4}{9}{AdaBoost results, reported at rounds 100, 300 and 500 (400 for RCV1).\relax }{table.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Train and Test Error for LazyBoost and Weight Trimming}{9}{subsection.1.A.2}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Performance for A6A\relax }}{9}{table.caption.18}}
\newlabel{tbl:perf-a6a}{{5}{9}{Performance for A6A\relax }{table.caption.18}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Performance for MNIST Digits\relax }}{9}{table.caption.19}}
\newlabel{tbl:perf-mnist}{{6}{9}{Performance for MNIST Digits\relax }{table.caption.19}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Performance for RCV1\relax }}{10}{table.caption.20}}
\newlabel{tbl:perf-rcv1}{{7}{10}{Performance for RCV1\relax }{table.caption.20}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Performance for SATIMAGE\relax }}{10}{table.caption.21}}
\newlabel{tbl:perf-satimage}{{8}{10}{Performance for SATIMAGE\relax }{table.caption.21}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Performance for W4A\relax }}{10}{table.caption.22}}
\newlabel{tbl:perf-w4a}{{9}{10}{Performance for W4A\relax }{table.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Different Tree Depths}{11}{subsection.1.A.3}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Different Tree Depths: Number of Assessments after 500 rounds\relax }}{11}{table.caption.23}}
\newlabel{DiffDepths}{{10}{11}{Different Tree Depths: Number of Assessments after 500 rounds\relax }{table.caption.23}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Information Gain}{12}{Appendix.1.B}}
\newlabel{eq:kl-term}{{1}{12}{Information Gain}{equation.1.B.1}{}}
\newlabel{thm:klub}{{1}{13}{KL Upper Bound}{lemma.1}{}}
\newlabel{lemma-kl}{{2}{13}{}{lemma.2}{}}
