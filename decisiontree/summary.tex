
\subsection{Conclusion}
In this chapter, we introduced an efficient exact greedy-optimal algorithm, \texttt{Adaptive-Pruning Boost}, for boosted decision trees. Our experiments on various datasets show that our algorithm use fewer total example assessments compared to the-state-of-the-art algorithm \texttt{Quick Boost}. We further showed that \texttt{Adaptive-Pruning Boost} almost matches the lower bound for its class of algorithms and the global lower bound for any algorithm.

In future work, we plan to add results for other tree splitting criteria
such as GINI and information gain.
We also plan to study additional boosting algorithms such as XGBoost
and to explore extending the approach to weak learners other than decision
trees.
