
\documentclass{article}

\input{notation}

\doublespacing

\begin{document}

\begin{titlepage}
    \begin{center}
        \vspace*{1cm}
 
        \Huge
        \textbf{On Multi-Armed Bandits Theory and Applications}
 
        \vspace{0.5cm}
        %\LARGE
        %Thesis Subtitle
 
        \vspace{1.5cm}
 
        \textbf{Maryam Aziz}
 
        \vfill
 
        In partial fulfillment of the requirements for the degree of \\
        Doctor of Philosophy
 
        \vspace{0.8cm}
 
        %\includegraphics[width=0.4\textwidth]{university}
 
        \Large
        Khoury College of Computer Science\\
        Northeastern University\\
        Boston, MA\\
        2018
 
    \end{center}
\end{titlepage}

 
  \thispagestyle{plain}
\begin{center}
    \Large
 To Mom, Zakia, and Dad, Abdul Ghafoor, who gave me the gift of life and nourished me.
\end{center}

 \newpage
 \thispagestyle{plain}
%\begin{center}
    %\Large
   %\large
\noindent
    \textbf{Abstract} \\
How would one go about choosing a near-best option from an effectively infinite set in finite time, with imperfect knowledge of the quality of the options? Such problems arise in computer science (e.g. online learning, reinforcement learning, hyper-parameter tuning, and recommender systems) and beyond. Consider drug testing, for example. One may have access to many candidate drugs (“arms”) but only resources to perform a limited number of tests, and yet one’s goal is to identify a “near-optimal” drug within the budget available. Such problems are well-modeled by variants of the classical multi-armed bandit problem.

We focus on the pure exploration version of the infinitely-armed bandit problem, wherein one seeks one of the best arms without penalty for trying sub-optimal arms along the way. The challenge is to quickly identify an arm with “near best” mean reward by repeatedly testing arms in some intelligent manner. We provide good general strategies to solve this problem in both the fixed budget setting, wherein one attempts to maximize performance with a certain number of tests, and the fixed confidence setting, wherein one attempts to minimize the number of tests needed to meet a certain performance target.

We also report two real-world applications. The first aims to train greedy-optimal boosted decision trees faster than state-of-the-art algorithms using a novel bandit-inspired algorithm. Our algorithm minimizes the number of training documents used to measure each possible decision tree split while ensuring that we identify the split which would score the best were all documents used. We show that our algorithm empirically almost matches a lower bound on algorithms of its class, and approaches a more general lower bound on the number of documents needed for any class of algorithms.

Lastly we report an application to dose finding in phase I clinical trials of cancer treatments. We develop a multi-objective bandit algorithm for balancing conflicting needs: the need to efficiently find the best dose level to treat future patients, the need to avoid giving trial patients unsafe doses, and the need to give trial patients large enough doses for effective treatment during the trial. Our method typically beats state-of-the-art methods in balancing the first two of these competing concerns well.
%\end{center}

 \newpage
 \thispagestyle{plain}
\begin{center}
   % \Large
    \vspace{0.9cm}
    \textbf{Acknowledgements} \\
``If we affirm one single moment, we thus affirm not only ourselves but all existence. For nothing is self-sufficient, neither in us ourselves nor in things; and if our soul has trembled with happiness and sounded like a harp string just once, all eternity was needed to produce this one event and in this single moment of affirmation all eternity was called good, redeemed, justified, and affirmed.''

— Nietzsche, Friedrich, The Will to Power
\end{center}

I would like to thank Javed Aslam, my advisor, for helping to bring out the best in me, and always having time for me if I needed it. I also would like to thank Emilie Kaufmann for not only being a great mentor for the last few years but also a gracious host during my time in France, and making me feel I am part of the bandit community.  I also would like to thank my collaborators, my committee members, those I love and my family. 


 \newpage

\fancyhead{}
\fancyhead[LO,CE]{\ifthenelse{\equal{\thesection}{0}}{}{Chapter \thesection}}
\fancyhead[RO,LE]{On Multi-Armed Bandit Theory and Applications}
\fancyfoot{}
\fancyfoot[LE,RO]{\thepage}
\fancyfoot[CO,RE]{Maryam Aziz}

%\doublespacing % Uncomment \usepackage{setspace}
 \thispagestyle{plain}
\tableofcontents
\listoffigures
\listoftables
% \thispagestyle{plain}
%\singlespacing % Reset spacing
\newpage


\section{Introduction}
We started out by exploring feature selection in boosting algorithms when
the number of features is too large to provide precise measurements of
the quality of each individual feature within a reasonable amount of time.
Can one improve upon uniform sampling (e.g., as in random forests)
in this setting?
For example, consider text classification from skip-grams, which consist of $n$ words co-occurring in any order within a window of size
$w>n$.
The number of skip-grams appearing in a large text corpus grows very 
quickly with $n$ and $w$, but some of these skip-grams are very good
features for classification.
How can we build a model which uses the top-scoring skip-grams when we
can not afford to measure the quality of each of them?
After some empirical work on such datasets, we decided to step back
and deal with the problem in the abstract, with the following research
question.

How would one go about choosing a near-best option from an effectively infinite set of options when one has a finite amount of time to make a decision and imperfect knowledge of the quality of the options?
Such problems are well-modeled by the ``pure exploration" variant of the classical multi-armed bandit problem.
We ultimately wrote two theoretical bandit model papers (one being wrapped up at the time of writing this thesis) addressing this
question,
and also a paper presenting an improved boosted decision tree training
algorithm using insights derived from my theoretical work.

A large portion of this thesis work is in the ``pure exploration" setting 
of infinitely armed bandits under no assumptions over the arm ``reservoir distribution."
This work permits the application of bandit models to a broader class of problems, where fewer assumptions are required for theoretical guarantees
to hold.
We examined this problem in both the fixed budget setting, wherein one attempts to maximize performance with limited resources
(e.g. CPU time or number of trials),
and the fixed confidence setting, wherein one attempts to minimize budget while meeting a quality constraint on the selected arm.

More formally, the goal of an infinitely armed bandit algorithm in the pure exploration setting is to return an $\epsilon$-good arm with probability at least $1-\delta$.
The complexity of the problem depends on $\epsilon, \delta$ and the so-called reservoir distribution $\nu$ from which the means of the arms are drawn i.i.d. While most previous work focus  on specific cases of $\nu$ we make no assumption on the reservoir.

Chapter~\ref{fixedconfidence} addresses the fixed confidence setting of this problem.
It proposes a new PAC-like $(\alpha,\epsilon,\delta)$ framework
within which an arm within $\epsilon$ of the top $\alpha$ fraction of
the reservoir is returned by an algorithm with probability at least
$1-\delta$.
In short, $\alpha$ specifies the quality of arm you want
(i.e. the probability of drawing a better arm),
$\epsilon$ indicates how much budget to spend on differentiating very
similar arms, and
$\delta$ provides the confidence guarantee.
We derived a sample complexity lower bound within this framework
and proposed an algorithm whose sample complexity is within a $\log(1/\delta)$ factor of our lower bound.
This  $\log(1/\delta)$ gap is commonly found in 
state-of-the-art algorithms for infinitely-armed bandits,
and it is not yet clear whether this gap can be closed without
assumptions about the reservoir distribution. This work in the fixed confidence setting was published in ALT 2018 ~\citep{aziz2018pure}.


In the fixed budget setting, Chapter~\ref{fixedbudget},
we proposed an algorithm based on successive halving,
which seeks the best of $n$ arms by running
$\log_2(n)$ rounds. In each round
the same number of samples is drawn from each surviving arm,
the half with worst empirical performance are removed,
and the number of samples per arm is doubled in the next round.
We show that running Successive Halving with $n$ randomly sampled arms and a budget of $n\log_2(n)$ pulls, where arms start being discarded after being pulled just once, beats state-of-the-art Hyperband. In exhaustive experimental studies, we showed that our algorithm is not only superior on most reservoir distributions (including those derived from finite-armed problems) but also against algorithms designed to make use of knowledge about the reservoirs which our algorithm does not have.
We also contribute an information theoretic lower bound for the infinite-armed bandit problem. As of the writing of this thesis, my collaborators, Kevin Jamieson and Javed Aslam, and I are working on proving the correctness of the algorithm. 

In Chapter~\ref{AP-Boost}, we return to the original research question of
efficiently training boosted decision trees with large feature sets~.
Inspired by our work in multi-armed bandits, we
developed a highly efficient algorithm for computing exact greedy-optimal decision
trees, outperforming the state-of-the-art Quick Boost~\citep{icml2013_appel13}.
We developed a framework for deriving lower bounds on the problem that applies to a wide family
of conceivable algorithms for the task (including our algorithm and Quick Boost),
and we demonstrated empirically on a variety of datasets that our algorithm
is near-optimal within this family of algorithms.
We further derived a lower bound applicable to any algorithm solving the
task, and we demonstrate that our algorithm empirically achieves
performance close to this best-achievable lower bound.
In this thesis, we provide results for trees split based on accuracy. My collaborators, Jesse Anderton and Javed Aslam, and I are preparing an update with GINI results.
%We are also considering adding results for
%XGBoost~\citep{Chen:2016:XST:2939672.2939785} as a final contribution.

Our multi-armed work also led to dose finding in clinical trials. My collaborators, Emilie Kaufmann and Marie-Karelle Riviere, and I  studied, Chapter~\ref{dosefinding}, the problem of finding the optimal dosage in a phase I clinical trial through the multi-armed bandit lens. We advocated the use of Thompson Sampling, a flexible algorithm that can accommodate different types of monotonicity assumptions on the toxicity and efficacy of the doses. We proposed two designs inspired by state-of-the-art multi armed bandit algorithms for which we provided finite-time upper bounds on the error probability or the number of sub-optimal dose selections, which is unprecedented for dose finding algorithms. Through a large simulation study, we then showed that variants of Thompson Sampling outperform state-of-the-art dose identification algorithms in different types of trials, in particular testing the most toxic doses fewer times and recommending the optimal doses more times.


\newpage
\section{The Fixed Confidence Setting}\label{fixedconfidence}
\input{fixedconfidence/aziz18}
\newpage
\section{The Fixed Budget Setting}\label{fixedbudget}
\input{fixedbudget/main.tex}
\section{Greedy-optimal Boosted Decision Trees}\label{AP-Boost}
\input{decisiontree/paper.tex}
\section{Dose Finding for Phase I Clinical Trials}\label{dosefinding}
\input{dosefinding/ClinicalTrials}
%\begin{figure}
%  \caption{Dummy figure}
%\end{figure}
%
%\begin{table}
%  \caption{Dummy table}
%\end{table}

%\newpage

% The appendix starts here, the list of figures and tables are auto-generated as 
% well.
%\addtocontents{toc}{\setcounter{tocdepth}{1}}
%\section{Another section}
%\subsection{Subsection}
%\subsubsection{Subsubsection}

%...

\addtocontents{toc}{\setcounter{tocdepth}{3}} % reset to default

\newpage

\small
\bibliographystyle{plainnat}
\bibliography{fixedconfidence/paper,fixedconfidence/biblioEmilie,refs,fixedbudget/refs, dosefinding/biblioBandits, decisiontree/paper, decisiontree/biblioEmilie}


%\bibliography{decisiontree/paper, decisiontree/biblioEmilie}
%\bibliographystyle{plainnat}
%\bibliographystyle{biometrics}
%\bibliography{biblioBandits}
%\bibliographystyle{plainnat}
%\bibliography{fixedbudget/refs}

\newpage

%\begin{appendix}
%  \listoffigures
%  \listoftables
%\end{appendix}
\end{document}